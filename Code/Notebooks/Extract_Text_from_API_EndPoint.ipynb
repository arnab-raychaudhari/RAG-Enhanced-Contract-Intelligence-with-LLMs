{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROGRAM OVERVIEW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this program to perform the following steps in sequence:\n",
    "\n",
    "1. **Retrieve** a set of PDFs from an API endpoint.\n",
    "2. **Optionally subset** the list of PDFs if only certain documents are needed.\n",
    "3. **Extract text** from the selected PDFs and **save** the output to a text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LIBRARIES AND PACKAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # type: ignore\n",
    "import os\n",
    "from PyPDF2 import PdfReader # type: ignore\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the **PDF LINKS** from the application **END POINT** and write them to a **PYTHON LIST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of PDF URLs:\n",
      "http://www.govinfo.gov/content/pkg/PLAW-117publ58/pdf/PLAW-117publ58.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-117publ103/pdf/PLAW-117publ103.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-115publ56/pdf/PLAW-115publ56.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-117publ328/pdf/PLAW-117publ328.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-117publ328/pdf/PLAW-117publ328.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-115publ123/pdf/PLAW-115publ123.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-116publ20/pdf/PLAW-116publ20.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-116publ94/pdf/PLAW-116publ94.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-116publ127/pdf/PLAW-116publ127.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-116publ136/pdf/PLAW-116publ136.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-116publ136/pdf/PLAW-116publ136.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-116publ139/pdf/PLAW-116publ139.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-116publ260/pdf/PLAW-116publ260.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-117publ2/pdf/PLAW-117publ2.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-117publ2/pdf/PLAW-117publ2.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-117publ43/pdf/PLAW-117publ43.pdf\n",
      "http://www.govinfo.gov/content/pkg/PLAW-117publ58/pdf/PLAW-117publ58.pdf\n"
     ]
    }
   ],
   "source": [
    "# Set the API endpoint\n",
    "\n",
    "#url = \"https://api.usaspending.gov/api/v2/agency/012/?fiscal_year=2023\"\n",
    "url = \"https://api.usaspending.gov/api/v2/agency/012/?fiscal_year=2024\"\n",
    "\n",
    "# Send a request to the API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the response is successful\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        data = response.json()  # Parse the JSON response\n",
    "        pdf_list = []\n",
    "\n",
    "        # Extract 'def_codes' if present in the response\n",
    "        def_codes = data.get('def_codes', [])\n",
    "        if def_codes:\n",
    "            # Iterate through 'def_codes' and extract 'urls'\n",
    "            for code in def_codes:\n",
    "                urls = code.get('urls')\n",
    "                if urls:  # Check if 'urls' is not None\n",
    "                    # Split 'urls' if it's a string and contains '|'\n",
    "                    if isinstance(urls, str):\n",
    "                        pdf_list.extend(urls.split('|'))\n",
    "                    # If 'urls' is a list, iterate and split if needed\n",
    "                    elif isinstance(urls, list):\n",
    "                        for url in urls:\n",
    "                            pdf_list.extend(url.split('|'))\n",
    "\n",
    "            # Display the consolidated list of PDF URLs\n",
    "            if pdf_list:\n",
    "                print(\"List of PDF URLs:\")\n",
    "                for pdf_url in pdf_list:\n",
    "                    print(pdf_url)\n",
    "            else:\n",
    "                print(\"No PDF URLs found in the 'def_codes' key.\")\n",
    "        else:\n",
    "            print(\"'def_codes' key not found in the response.\")\n",
    "    except ValueError:\n",
    "        print(\"Error: Unable to parse JSON response.\")\n",
    "else:\n",
    "    print(\"Response content:\", response.text)\n",
    "    print(f\"Failed to fetch data from API. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONSOLIDATED TEXT FILE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the list of PDF URLs, download each PDF to a local directory, and then extract the text from each PDF. Finally, compile the extracted text into a single consolidated text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/1_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/3_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/A_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/AAB_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/AAC_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/C_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/E_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/I_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/M_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/N_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/O_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/O_document_2.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/O_document_3.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/O_document_4.pdf\n",
      "No valid 'urls' found in def_codes entry: {'code': 'Q', 'public_law': 'Not Designated Nonemergency/Emergency/Disaster/Wildfire Suppression', 'title': 'Not Designated Nonemergency/Emergency/Disaster/Wildfire Suppression', 'urls': None, 'disaster': None}\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/V_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/X_document_1.pdf\n",
      "Downloaded PDF: USSP_DOWNLOAD_20241031_015055/Z_document_1.pdf\n",
      "All PDF content has been written to /Users/arnabraychaudhari/Documents/6317/Project_LLM_and_RAG_2024_GWU/USSP_DOWNLOAD_20241031_015055/USSP_Consolidated_20241031_015055.txt.\n"
     ]
    }
   ],
   "source": [
    "# If you want to subset the PDFs from API EndPoint, review and update the following list\n",
    "\n",
    "# pdf_list = [\n",
    "#      \"http://www.govinfo.gov/content/pkg/PLAW-117publ58/pdf/PLAW-117publ58.pdf\"\n",
    "#  ]\n",
    "#pdf_list = [\"http://www.govinfo.gov/content/pkg/PLAW-115publ56/pdf/PLAW-115publ56.pdf\"]\n",
    "\n",
    "# Get the current date and timestamp\n",
    "current_datetime = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Folder to save PDFs with date and timestamp\n",
    "pdf_folder = f'USSP_DOWNLOAD_{current_datetime}' #change the prefix as per your requirement\n",
    "os.makedirs(pdf_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Text file to store extracted content with date and timestamp in the path. Change the directory as per your requirement.\n",
    "output_text_file = f'/Users/arnabraychaudhari/Documents/6317/Project_LLM_and_RAG_2024_GWU/{pdf_folder}/USSP_Consolidated_{current_datetime}.txt'\n",
    "\n",
    "os.makedirs(os.path.dirname(output_text_file), exist_ok=True)\n",
    "\n",
    "# Fetch data from API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Open the text file in write mode\n",
    "with open(output_text_file, 'w') as text_file:\n",
    "    # Check if the response is successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Parse the JSON data from the API response\n",
    "\n",
    "        # Extract the \"def_codes\" key containing the PDF URLs\n",
    "        if 'def_codes' in data:\n",
    "            def_codes = data['def_codes']\n",
    "            for code in def_codes:\n",
    "                if 'urls' in code and code['urls']:  # Check if 'urls' exists and is not None\n",
    "                    # Split the URLs if they are separated by '|'\n",
    "                    pdf_urls = code['urls'].split('|') if '|' in code['urls'] else [code['urls']]\n",
    "                    for i, pdf_url in enumerate(pdf_urls):\n",
    "                        # Ensure the URL starts with http or https\n",
    "                        if not pdf_url.startswith('http'):\n",
    "                            print(f\"Invalid URL: {pdf_url}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Check if the PDF URL is in the predefined list\n",
    "                        if pdf_url not in pdf_list:\n",
    "                            print(f\"Skipping PDF not in the list: {pdf_url}\")\n",
    "                            continue\n",
    "\n",
    "                        pdf_name = f\"{pdf_folder}/{code['code']}_document_{i+1}.pdf\"  # Save PDFs with a name based on the code and index\n",
    "                        try:\n",
    "                            # Download each PDF\n",
    "                            pdf_response = requests.get(pdf_url)\n",
    "                            if pdf_response.status_code == 200:\n",
    "                                # Write the PDF content to the file\n",
    "                                with open(pdf_name, 'wb') as pdf_file:\n",
    "                                    pdf_file.write(pdf_response.content)\n",
    "                                print(f\"Downloaded PDF: {pdf_name}\")\n",
    "                                \n",
    "                                # Extract text from the downloaded PDF\n",
    "                                with open(pdf_name, 'rb') as file:\n",
    "                                    reader = PdfReader(file)\n",
    "                                    for page_num in range(len(reader.pages)):\n",
    "                                        page = reader.pages[page_num]\n",
    "                                        text = page.extract_text()\n",
    "                                        if text:  # Only write if text was extracted\n",
    "                                            text_file.write(f\"\\n\\n[Extract from {pdf_name} - Page {page_num+1}]\\n\")\n",
    "                                            text_file.write(text)\n",
    "                            else:\n",
    "                                print(f\"Failed to download {pdf_url}, Status Code: {pdf_response.status_code}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error downloading {pdf_url}: {e}\")\n",
    "                else:\n",
    "                    print(f\"No valid 'urls' found in def_codes entry: {code}\")\n",
    "        else:\n",
    "            print(\"No def_codes or URLs found in the API response.\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from API. Status code: {response.status_code}\")\n",
    "\n",
    "print(f\"All PDF content has been written to {output_text_file}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
